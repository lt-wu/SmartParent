from PIL import Image
import json
import os
import streamlit as st
import faiss
import numpy as np
import pandas as pd
import pickle

import autogen
from autogen import OpenAIWrapper
from tools.get_llm_config import get_llm_config
from tools.askllm import ask_gpt4o_to_summarize
from tools.fetchEmail import fetchEmail
#from tools.eventCreate import eventCreate
from tools.email import create_event
from tools.model import OrchestratorAgent
from tools.query_faiss_db import generate_query_embedding,  query_faiss_db, rewrite_query_based_on_results, query_faiss_db

from dotenv import load_dotenv
load_dotenv(dotenv_path="./.env")

# Environment variables
api_key = os.getenv("AZURE_OPENAI_API_KEY")
EMBEDDING_DIMENSION = 512
# Global variable for FAISS index



engineer = autogen.AssistantAgent(
    name="Engineer",
    llm_config=get_llm_config("gpt-4o"),
    system_message="""
    I'm Engineer. I'm expert in python programming. I'm executing code tasks required by Admin.
    """,
)

emailParser = autogen.AssistantAgent(
    name="EmailReader",
    llm_config=get_llm_config("gpt-4o"),
    system_message="""
    Read latest newsletter from mail box, save to image file.
    """,
    code_execution_config={"use_docker":False}
    )


# Admin Agent
user_proxy = autogen.UserProxyAgent(
    name="Admin",
    human_input_mode="NEVER",  # Automatically proceed without manual input
    code_execution_config=False,
)

@user_proxy.register_for_execution()
@engineer.register_for_llm(description="Get email")
def emailReader():
    print("Executing emailReader function...")
    result = fetchEmail()
    with st.sidebar:
        st.success(f"‚úÖ Newsletter fetched successfully")
    return result


@user_proxy.register_for_execution()
@engineer.register_for_llm(description="Parse email body")
def emailParser():
    global parsed_email_data
    agent = OrchestratorAgent(api_key)

    images = [Image.open("./data/out1.jpg")]
    # images = [
    #     Image.open("./data/out1.jpg"),
    #     Image.open("./data/out5.jpg")
    # ]
    
    message_histories = [[{"role": "user", "content": "Hint: School event"}]]


    query = f"""
        
        What are important dates and all the events during 2025 year, their respective start times, and end times from the email? 
        Please format the response according to the following template, don't show "," in subject and description, 
        all the dates are scheduled in 2025, 
        if don't have time in the conetxt just set 12:00:00 am for both starttime and endtime
        if don't have end time in the conetxt just set end time = start time

        data:
        [
        {{
            "subject": "Event Subject",
            "start_date": "YYYY-MM-DDTHH:MM:SS",
            "end_date": "YYYY-MM-DDTHH:MM:SS",
            "description": "Brief description of the event"
        }},
        {{
            "subject": "Event Subject",
            "start_date": "YYYY-MM-DDTHH:MM:SS",
            "end_date": "YYYY-MM-DDTHH:MM:SS",
            "description": "Brief description of the event"
        }}
        ]

        """

    # query = f"""
    #     What are the important points, main events, their respective start times, and end times from the email?  
    #     """
    
    response = agent.batch_generate_response([query], images, message_histories)
    if response is None or len(response) == 0:
        st.error("No response generated by the agent")
        raise ValueError("No response generated by the agent.")
        
    with st.sidebar:
        st.success(f"‚úÖ Newsletter parsed successfully")
        parsed_email_data = response
        return parsed_email_data


@user_proxy.register_for_execution()
@engineer.register_for_llm(description="Summarize paired structured data and text.")
def pairedDataSummarizer():
    """
    COnvert paired structured data (from emailParser) using GPT-4o to json formate.

    Args:
        structured_data (Dict[str, Any]): Structured data (e.g., JSON or dictionary).
        additional_text (str): Additional text to summarize along with the structured data.

    Returns:
        str: Summarized output.
        
        Events:
        [
        {{
            "subject": "Event Subject",
            "start_date": "YYYY-MM-DDTHH:MM:SS",
            "end_date": "YYYY-MM-DDTHH:MM:SS",
            "description": "Brief description of the event"
        }},
        {{
            "subject": "Event Subject",
            "start_date": "YYYY-MM-DDTHH:MM:SS",
            "end_date": "YYYY-MM-DDTHH:MM:SS",
            "description": "Brief description of the event"
        }}
        ]

    """

    global parsed_email_data, json_res

    # Convert structured data to JSON string for better readability
    #structured_data_json = json.dumps(structured_data, indent=4)

    response = ask_gpt4o_to_summarize(api_key, parsed_email_data)
    with st.sidebar:
        st.success(f"‚úÖ Converted structured data to JSON string successfully")
    # Parse the cleaned response to JSON
    json_res = response
    return json_res
        


@user_proxy.register_for_execution()
@engineer.register_for_llm(description="Create Google Calendar events from summarized data.")
def createEventFromSummary():
    """
    Create events on Google Calendar using the summarized JSON data.
    """
    global json_res
    # Ensure `pairedDataSummarizer` has been executed and data is available
    if json_res is None:
        raise ValueError("No summarized data available. Run 'pairedDataSummarizer' first.")

    # Check if `json_res` is a valid dictionary
    try:
        parsed_json = json.loads(json_res) if isinstance(json_res, str) else json_res
        print(f"Parsed JSON: {parsed_json}")
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
        raise ValueError(f"Invalid JSON format: {json_res}")

    # Create events on Google Calendar
    if "data" not in parsed_json:
        raise ValueError("'Important_dates_or_Event' key not found in the JSON data.")

    for event in parsed_json["data"]:
        print("EVENT ", event)
        event_data = {
            "summary": event["subject"],
            "description": event["description"],
            "start": {"dateTime": event["start_date"], "timeZone": "America/Los_Angeles"},
            "end": {"dateTime": event["end_date"], "timeZone": "America/Los_Angeles"},
        }
        try:
            create_event(event_data)
        except Exception as e:
            print(f"Error creating event: {e}")

    with st.sidebar:
        st.success(f"‚úÖ All events created on calendar successfully.")
        df = pd.DataFrame(json_res['data']).reset_index(drop=True)
        st.title("Main Events")
        # Add a new column with üìÖ concatenated to the subject column
        print("Add a new column", df)
        df['subject'] =  'üìÖ' + df['subject']
        print("After a new column", df)

        # Display the table using Streamlit
        st.dataframe(df)  # Interactive table (scrollable and sortable)

    return "All events created successfully."


FAISS_INDEX_PATH = "./faiss_index.bin"
METADATA_PATH = "./metadata.pkl"
faiss_index = None
@user_proxy.register_for_execution()
@engineer.register_for_llm(description="Save JSON response into FAISS database for efficient search and retrieval.")
def save_to_faiss_db():
    """
    Save the summarized JSON response (`json_res`) into FAISS database.
    """
    global json_res, faiss_index, metadata_store

    # Ensure `pairedDataSummarizer` has been executed and data is available
    if json_res is None:
        raise ValueError("No summarized data available. Run 'pairedDataSummarizer' first.")

    # Convert JSON response to embeddings (e.g., using a hypothetical embedding function)
    try:
        parsed_json = json.loads(json_res) if isinstance(json_res, str) else json_res
        print(f"Parsed JSON: {parsed_json}")
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
        raise ValueError(f"Invalid JSON format: {json_res}")

    if "data" not in parsed_json:
        raise ValueError("'data' key not found in the JSON response.")

    # Example embedding generation (replace this with actual embedding logic)
    embedding_dimension = 512  # Example dimension for embeddings
    event_embeddings = []
    event_metadata = []

    for event in parsed_json["data"]:
        # Mock embedding creation: Replace with actual embedding generation logic
        event_embedding = np.random.rand(embedding_dimension).astype('float32')  # Dummy embedding
        event_embeddings.append(event_embedding)
        event_metadata.append(event)

    # Initialize FAISS index if not already done
    if faiss_index is None:
        faiss_index = faiss.IndexFlatL2(embedding_dimension)  # L2 distance index
        with st.sidebar:
            st.info("Initialized FAISS index.")

    # Add embeddings to FAISS index
    faiss_index.add(np.array(event_embeddings))

    # Optionally, store metadata separately (e.g., using a dictionary or database)
    global metadata_store
    metadata_store = event_metadata

    #faiss.write_index(faiss_index, FAISS_INDEX_PATH)
    print(f"[DEBUG] FAISS index saved to {FAISS_INDEX_PATH}")

    # Save metadata to disk
    # with open(METADATA_PATH, "wb") as f:
    #     pickle.dump(metadata_store, f)

    # Debugging: Print FAISS index size and metadata count
    print(f"[DEBUG] FAISS Index Size: {faiss_index.ntotal}")
    print(f"[DEBUG] Metadata Count: {len(metadata_store)}")

    with st.sidebar:
        st.success("FAISS database updated with event embeddings.")
    return "FAISS vector DB updated successfully."


# Example function to generate a query embedding (replace with actual embedding logic)
def generate_query_embedding(query_text):
    """
    Generate an embedding for the given query text.

    Args:
        query_text (str): The text to generate the embedding for.

    Returns:
        np.array: Generated embedding.
    """
    embedding_dimension = 512
    return np.random.rand(embedding_dimension).astype('float32')  # Dummy embedding

# Ensure Streamlit session state is initialized
if "groupchat_completed" not in st.session_state:
    st.session_state.groupchat_completed = False  # Initialize to False


# Define the group chat
groupchat = autogen.GroupChat(
    agents=[engineer, user_proxy],
    messages=[
        {"role": "Admin", "content": "Run the emailReader function to fetch the latest newsletter."},
        {"role": "Admin", "content": "Run the emailParser function to fetch the latest newsletter."},
        {"role": "Admin", "content": "Run the pairedDataSummarizer function to summarize paired data."},
        {"role": "Admin", "content": "Run the createEventFromSummary function to create calendar events."},
        {"role": "Admin", "content": "Run the save_to_faiss_db function to save data to Vector DBs."},

    ],  # Initial message
    max_round=10,
    enable_clear_history=True,
)

manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=get_llm_config("gpt-4o"))

# Run GroupChat only if not completed
if not st.session_state.groupchat_completed:
    try:
        with st.sidebar:
            st.write("Starting the GroupChat workflow...")
        chat_result = user_proxy.initiate_chat(
            manager,
            message="""Start code process"""
        )
        st.session_state.groupchat_completed = True  # Mark as completed upon success
        with st.sidebar:
            st.success("‚úÖ GroupChat workflow completed successfully!")
    except Exception as e:
        st.error(f"‚ùå An error occurred while running the GroupChat: {e}")



def load_faiss_db():
    """
    Load the FAISS index and metadata from disk.
    """
    global faiss_index, metadata_store

    # Load FAISS index
    if os.path.exists(FAISS_INDEX_PATH):
        faiss_index = faiss.read_index(FAISS_INDEX_PATH)
        print(f"[DEBUG] FAISS index loaded from {FAISS_INDEX_PATH}")
    else:
        print("[WARNING] FAISS index file not found. Initialize the FAISS database.")

    # Load metadata
    if os.path.exists(METADATA_PATH):
        with open(METADATA_PATH, "rb") as f:
            metadata_store = pickle.load(f)
        print(f"[DEBUG] Metadata loaded from {METADATA_PATH}")
    else:
        print("[WARNING] Metadata file not found. Initialize the FAISS database.")
    return faiss_index, metadata_store



# Streamlit Chat UI
st.title("‚ú® Smart Parent")
st.write("Ask me about newsletter!")

# Create a chat container
if "messages" not in st.session_state:
    st.session_state.messages = []  # Initialize chat messages

# Display all previous messages
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).markdown(msg["content"])

# User input for the query
if user_input := st.chat_input("Ask a question about events..."):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").markdown(user_input)

    # Query FAISS database
    faiss_index, metadata_store = load_faiss_db()
    print(f"[DEBUG] FAISS Index Size: {faiss_index.ntotal}")
    print(f"[DEBUG] Metadata Count: {len(metadata_store)}")


    if faiss_index is not None and metadata_store is not None:
        query_embedding = generate_query_embedding(user_input)
        results = query_faiss_db(faiss_index, metadata_store, query_embedding)
        refined_results = rewrite_query_based_on_results(user_input, results)
        print("refined_results", refined_results)

        # Generate AI response based on results
        # if refined_results:
        #     response = f"Here are the most relevant events:\n\n"
        #     for result in refined_results:
        #         response += f"- **{result['subject']}**\n  {result['description']}\n  Start: {result['start_date']}\n  End: {result['end_date']}\n\n"
        # else:
        #     response = "I couldn't find any relevant events in the database."

        # Add AI response to chat history
        st.session_state.messages.append({"role": "assistant", "content": refined_results})
        st.chat_message("assistant").markdown(refined_results)
    else:
        # If FAISS database is not ready
        error_message = "FAISS database is not initialized. Please run the workflow to populate the database."
        st.session_state.messages.append({"role": "assistant", "content": error_message})
        st.chat_message("assistant").markdown(error_message)